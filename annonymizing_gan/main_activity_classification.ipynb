{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fossil-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(100)\n",
    "X,y_hr,y_participant,y_activity,y_time = pickle.load(open('../data/heart_rate_tabular_data_ppg_dalia.p','rb'))\n",
    "activity_dict1 = {'No Label':-1,'Sitting':0,'Stairs':1,'Soccer':2,\n",
    "                'Cycling':3,'Driving':4,'Lunch':-1,'Walking':5,\n",
    "                'Working':-1}\n",
    "y_activity = np.array([activity_dict1[a] for a in y_activity]) \n",
    "X = X[:,:,1:].reshape(-1,256,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "attempted-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_val_data(X_hr,y_participant,y_activity,y_time):\n",
    "    all_data = pd.DataFrame({'x':np.arange(X_hr.shape[0]),'y':y_participant,'activity':y_activity,'time':y_time})\n",
    "    train_percentage = .7\n",
    "    val_percentage = train_percentage+.1\n",
    "    def split_data(df):\n",
    "        df = df.sort_values('time').reset_index(drop=True)\n",
    "        n = df.shape[0]\n",
    "        train_index = df['x'].values[:int(n*train_percentage)]\n",
    "        val_index = df['x'].values[int(n*train_percentage):int(n*val_percentage)]\n",
    "        test_index = df['x'].values[int(n*val_percentage):]\n",
    "        return pd.DataFrame({'train':[list(train_index)],'val':[list(val_index)],'test':[list(test_index)]})\n",
    "\n",
    "    index_df = all_data.groupby(['y','activity'],as_index=False).apply(split_data)\n",
    "\n",
    "    from functools import reduce\n",
    "    train_index = np.array(reduce(lambda a,b:a+b,index_df['train'].values))\n",
    "    val_index = np.array(reduce(lambda a,b:a+b,index_df['val'].values))\n",
    "    test_index = np.array(reduce(lambda a,b:a+b,index_df['test'].values))\n",
    "\n",
    "\n",
    "    train_x,train_y = X_hr[train_index],y_participant[train_index]\n",
    "    val_x,val_y = X_hr[val_index],y_participant[val_index]\n",
    "    test_x,test_y = X_hr[test_index],y_participant[test_index]\n",
    "    return train_x,train_y,val_x,val_y,test_x,test_y,train_index,test_index,val_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "judicial-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "window_size = 8\n",
    "n_timesteps = window_size*32\n",
    "n_channels = 3\n",
    "train_x,train_y,val_x,val_y,test_x,test_y,train_index,test_index,val_index  = get_train_test_val_data(X[:,:,:],\n",
    "                                                                                                      y_participant,\n",
    "                                                                                                      y_activity,\n",
    "                                                                                                      y_time)\n",
    "train_y = y_activity[train_index]\n",
    "test_y = y_activity[test_index]\n",
    "val_y = y_activity[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "welsh-plumbing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split,LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import numpy as np\n",
    "# %matplotlib notebook\n",
    "from tensorflow.keras.layers import Conv1D,BatchNormalization,Dropout,Input,MaxPooling1D,Flatten,Dense,Input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "indoor-cooking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 256, 100)          3100      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 256, 100)          100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 128, 100)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128, 100)          400       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 128, 100)          100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 64, 100)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 64, 100)           100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 32, 100)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 100)           400       \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 32, 200)           200200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 16, 200)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16, 50)            20050     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 4, 50)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 50)             200       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 50)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               24120     \n",
      "_________________________________________________________________\n",
      "embedding (Lambda)           (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 726       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 42        \n",
      "=================================================================\n",
      "Total params: 550,738\n",
      "Trainable params: 549,638\n",
      "Non-trainable params: 1,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model(input_shape=(256,3),act='tanh',loss=\"mae\",opt='adam',n_classes=1):\n",
    "    model =  Sequential()\n",
    "    model.add(Conv1D(100,10,input_shape=input_shape,activation='linear',kernel_initializer='normal',padding='same'))\n",
    "    model.add(Conv1D(100,10,input_shape=input_shape,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Conv1D(100,10,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Conv1D(100,10,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(200,10,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Conv1D(50,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_classes*20,activation=None,kernel_initializer='normal'))\n",
    "    model.add(tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1),name='embedding'))\n",
    "    model.add(Dense(n_classes,activation='sigmoid',kernel_initializer='normal'))\n",
    "    model.add(Dense(n_classes,activation='softmax',kernel_initializer='normal'))\n",
    "    model.compile(loss=loss,optimizer=opt)\n",
    "    return model\n",
    "model =  get_model(n_classes=6)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-trader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1756/1756 [==============================] - 26s 13ms/step - loss: 1.5940 - val_loss: 1.8919\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.89192, saving model to ../model_files/activity_estimator_8_secs_dalia.hdf5\n",
      "Epoch 2/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 1.2399 - val_loss: 1.9956\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.89192\n",
      "Epoch 3/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 1.0143 - val_loss: 1.7638\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.89192 to 1.76380, saving model to ../model_files/activity_estimator_8_secs_dalia.hdf5\n",
      "Epoch 4/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.7620 - val_loss: 0.6972\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.76380 to 0.69717, saving model to ../model_files/activity_estimator_8_secs_dalia.hdf5\n",
      "Epoch 5/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.5860 - val_loss: 0.7173\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.69717\n",
      "Epoch 6/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.4913 - val_loss: 1.2044\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.69717\n",
      "Epoch 7/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.4444 - val_loss: 1.0945\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.69717\n",
      "Epoch 8/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.3964 - val_loss: 0.9649\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.69717\n",
      "Epoch 9/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.3757 - val_loss: 1.2729\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.69717\n",
      "Epoch 10/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.3076 - val_loss: 0.8226\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.69717\n",
      "Epoch 11/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.2751 - val_loss: 1.3118\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.69717\n",
      "Epoch 12/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.2440 - val_loss: 1.7350\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.69717\n",
      "Epoch 13/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.2047 - val_loss: 0.8669\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.69717\n",
      "Epoch 14/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.1823 - val_loss: 2.0407\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.69717\n",
      "Epoch 15/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.1698 - val_loss: 1.7017\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.69717\n",
      "Epoch 16/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.1522 - val_loss: 1.5931\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.69717\n",
      "Epoch 17/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.1451 - val_loss: 0.8955\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.69717\n",
      "Epoch 18/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.1338 - val_loss: 0.4652\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.69717 to 0.46521, saving model to ../model_files/activity_estimator_8_secs_dalia.hdf5\n",
      "Epoch 19/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.1222 - val_loss: 0.4502\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.46521 to 0.45020, saving model to ../model_files/activity_estimator_8_secs_dalia.hdf5\n",
      "Epoch 20/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.1225 - val_loss: 0.4880\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.45020\n",
      "Epoch 21/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.1159 - val_loss: 6.2244\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.45020\n",
      "Epoch 22/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.1132 - val_loss: 0.6333\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.45020\n",
      "Epoch 23/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0964 - val_loss: 1.0240\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.45020\n",
      "Epoch 24/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.1006 - val_loss: 0.7458\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.45020\n",
      "Epoch 25/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0905 - val_loss: 0.3140\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.45020 to 0.31399, saving model to ../model_files/activity_estimator_8_secs_dalia.hdf5\n",
      "Epoch 26/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0882 - val_loss: 1.6222\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.31399\n",
      "Epoch 27/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0851 - val_loss: 0.5234\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.31399\n",
      "Epoch 28/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0946 - val_loss: 1.2024\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.31399\n",
      "Epoch 29/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0675 - val_loss: 0.5234\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.31399\n",
      "Epoch 30/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0726 - val_loss: 0.3306\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.31399\n",
      "Epoch 31/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0701 - val_loss: 0.5882\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.31399\n",
      "Epoch 32/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0649 - val_loss: 0.5509\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.31399\n",
      "Epoch 33/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0574 - val_loss: 0.7157\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.31399\n",
      "Epoch 34/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0642 - val_loss: 0.1874\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.31399 to 0.18737, saving model to ../model_files/activity_estimator_8_secs_dalia.hdf5\n",
      "Epoch 35/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0664 - val_loss: 2.5326\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.18737\n",
      "Epoch 36/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0819 - val_loss: 2.2029\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.18737\n",
      "Epoch 37/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0537 - val_loss: 0.1705\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.18737 to 0.17052, saving model to ../model_files/activity_estimator_8_secs_dalia.hdf5\n",
      "Epoch 38/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0576 - val_loss: 0.3360\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.17052\n",
      "Epoch 39/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0549 - val_loss: 0.7495\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.17052\n",
      "Epoch 40/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0504 - val_loss: 0.6698\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.17052\n",
      "Epoch 41/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0554 - val_loss: 1.0082\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.17052\n",
      "Epoch 42/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0525 - val_loss: 1.3107\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.17052\n",
      "Epoch 43/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0445 - val_loss: 0.3689\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.17052\n",
      "Epoch 44/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0612 - val_loss: 0.7166\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.17052\n",
      "Epoch 45/100\n",
      "1756/1756 [==============================] - 23s 13ms/step - loss: 0.0465 - val_loss: 0.2301\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.17052\n",
      "Epoch 46/100\n",
      "1663/1756 [===========================>..] - ETA: 1s - loss: 0.0397"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "n_classes = len(np.unique(y_activity))\n",
    "model = get_model(n_classes=n_classes,loss='sparse_categorical_crossentropy')\n",
    "filepath = '../model_files/activity_estimator_8_secs_dalia.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min',save_weights_only=False)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=40)\n",
    "callbacks_list = [es,checkpoint]\n",
    "history = model.fit(train_x,train_y,validation_data=(val_x,val_y), epochs=100, batch_size=10,verbose=1,\n",
    "      callbacks=callbacks_list,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stable-hearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.99      0.84       451\n",
      "           1       0.55      0.70      0.62       325\n",
      "           2       0.79      0.18      0.29       229\n",
      "           3       1.00      0.39      0.56       345\n",
      "           4       0.62      0.93      0.74       683\n",
      "           5       0.86      0.47      0.61       474\n",
      "\n",
      "    accuracy                           0.68      2507\n",
      "   macro avg       0.76      0.61      0.61      2507\n",
      "weighted avg       0.74      0.68      0.65      2507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(filepath)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(val_y,model.predict(val_x).argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-distribution",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
