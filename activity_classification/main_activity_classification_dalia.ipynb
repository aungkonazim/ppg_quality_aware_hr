{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unlikely-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "X,y_hr,y_participant,y_activity = pickle.load(open('../data/heart_rate_tabular_data_ppg_dalia.p','rb'))\n",
    "activity_dict1 = {'No Label':-1,'Sitting':0,'Stairs':1,'Soccer':2,\n",
    "                'Cycling':3,'Driving':4,'Lunch':-1,'Walking':5,\n",
    "                'Working':-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sized-referral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25144,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X[:,:,1:].reshape(-1,256,3)\n",
    "y_activity = np.array([activity_dict1[a] for a in y_activity])\n",
    "# y_activity = OneHotEncoder().fit_transform(y_activity.reshape(-1,1)).todense()\n",
    "y_activity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "professional-causing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split,LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import numpy as np\n",
    "# %matplotlib notebook\n",
    "from tensorflow.keras.layers import Conv1D,BatchNormalization,Dropout,Input,MaxPooling1D,Flatten,Dense,Input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lesser-footwear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 256, 100)          3100      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 256, 100)          100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 128, 100)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128, 100)          400       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 128, 100)          100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 64, 100)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 64, 100)           100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 32, 100)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 100)           400       \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 32, 200)           200200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 16, 200)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 200)           800       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 16, 50)            20050     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 4, 50)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 50)             200       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 50)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               24120     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 726       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 42        \n",
      "=================================================================\n",
      "Total params: 550,738\n",
      "Trainable params: 549,638\n",
      "Non-trainable params: 1,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model(input_shape=(256,3),act='tanh',loss=\"mae\",opt='adam',n_classes=1):\n",
    "    model =  Sequential()\n",
    "    model.add(Conv1D(100,10,input_shape=input_shape,activation='linear',kernel_initializer='normal',padding='same'))\n",
    "    model.add(Conv1D(100,10,input_shape=input_shape,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Conv1D(100,10,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Conv1D(100,10,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(200,10,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Conv1D(50,2,activation='relu',kernel_initializer='normal',padding='same'))\n",
    "    model.add(MaxPooling1D(4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_classes*20,activation='relu',kernel_initializer='normal'))\n",
    "#     model.add(tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1),name='embedding'))\n",
    "    model.add(Dense(n_classes,activation=None,kernel_initializer='normal'))\n",
    "    model.add(Dense(n_classes,activation='softmax',kernel_initializer='normal'))\n",
    "    model.compile(loss=loss,optimizer=opt)\n",
    "    return model\n",
    "model =  get_model(n_classes=6)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "n_classes = len(np.unique(y_activity))\n",
    "model = get_model(n_classes=n_classes,loss='sparse_categorical_crossentropy')\n",
    "filepath = '../model_files/activity_estimator_8_secs_dalia.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min',save_weights_only=False)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=20)\n",
    "callbacks_list = [es,checkpoint]\n",
    "train_x,val_x,train_y,val_y = train_test_split(X,y_activity,test_size=.2,stratify=y_activity)\n",
    "history = model.fit(train_x,train_y,validation_data=(val_x,val_y), epochs=100, batch_size=100,verbose=1,\n",
    "      callbacks=callbacks_list,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "logo = LeaveOneGroupOut()\n",
    "y_final = []\n",
    "y_final_pred = []\n",
    "y_activity_final = []\n",
    "y_participant_final = []\n",
    "n_classes = len(np.unique(y_activity))\n",
    "for train_index, test_index in logo.split(X, y_hr, y_participant.reshape(-1)):\n",
    "    train_x, test_x = X[train_index], X[test_index]\n",
    "    train_y, test_y = y_activity[train_index], y_activity[test_index]\n",
    "    train_x, val_x, train_y, val_y = train_test_split(train_x,train_y,test_size = 0.1,stratify=train_y)\n",
    "    print((train_x.shape, train_y.shape), \n",
    "          (val_x.shape, val_y.shape),\n",
    "          (test_x.shape, test_y.shape))    \n",
    "    model = get_model(n_classes=n_classes,loss='sparse_categorical_crossentropy')\n",
    "    filepath = '../model_files/temp_activity_estimator.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min',save_weights_only=True)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=0,patience=20)\n",
    "    callbacks_list = [es,checkpoint]\n",
    "    history = model.fit(train_x,train_y,validation_data=(val_x,val_y), epochs=100, batch_size=50,verbose=0,\n",
    "          callbacks=callbacks_list,shuffle=True)\n",
    "    model.load_weights(filepath)\n",
    "    test_y_pred = model.predict(test_x).argmax(axis=1)\n",
    "    y_final.extend(list(test_y))\n",
    "    y_final_pred.extend(list(test_y_pred))\n",
    "    y_participant_final.extend(list(y_participant[test_index]))\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(test_y,test_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([y_final,y_final_pred,y_participant_final],open('../har_preciction_results.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = model.predict(test_x)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y,y_prediction.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final,y_final_pred,y_participant_final = pickle.load(open('../har_preciction_results.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.DataFrame({'original':y_final,'prediction':y_final_pred,'participant':y_participant_final})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_dict_reverse = {activity_dict1[key]:key for key in activity_dict1.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "result  = data.groupby(['participant','original'],as_index=False).apply(lambda a:pd.Series({'accuracy':accuracy_score(a['original'],a['prediction'])}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "result['activity'] = result['original'].apply(lambda a:activity_dict_reverse[a])\n",
    "result['participant'] = result['participant'].apply(lambda a:'p'+str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':30})\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(x='participant',y='accuracy',data=result)\n",
    "plt.ylim([0,1])\n",
    "plt.savefig('./images/partcipant_wise_performance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':30})\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(x='activity',y='accuracy',data=result)\n",
    "plt.ylim([0,1])\n",
    "plt.savefig('./images/activity_wise_performance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-moral",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
