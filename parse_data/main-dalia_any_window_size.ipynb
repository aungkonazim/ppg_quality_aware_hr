{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzipping Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "directory = '../data/data.zip'\n",
    "output_directory = '../data/'\n",
    "\n",
    "os.listdir(output_directory)\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(directory, 'r') as zip_ref:\n",
    "    zip_ref.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Individual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ecgdetectors import Detectors\n",
    "from scipy import signal\n",
    "from scipy.stats import skew,kurtosis,iqr\n",
    "import pickle\n",
    "from peak_valley import compute_peak_valley\n",
    "from respiration_feature import rip_cycle_feature_computation\n",
    "filelists = ['../data/PPG_FieldStudy/'+a+'/'+a+'.pkl' for a in os.listdir('../data/PPG_FieldStudy/') if a[-1] not in ['s','f']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.signal import find_peaks\n",
    "plt.close('all')\n",
    "def filter_data(X,\n",
    "                Fs=64,\n",
    "                low_cutoff=.5,\n",
    "                high_cutoff=3.0,\n",
    "                filter_order=1):\n",
    "    \"\"\"\n",
    "    Bandpass Filter of single channel\n",
    "\n",
    "    :param X: input data\n",
    "    :param Fs: sampling freq.\n",
    "    :param low_cutoff: low passband\n",
    "    :param high_cutoff: high passband\n",
    "    :param filter_order: no of taps in FIR filter\n",
    "\n",
    "    :return: filtered version of input data\n",
    "    \"\"\"\n",
    "    X1 = X.reshape(-1,1)\n",
    "    X1 = signal.detrend(X1,axis=0,type='constant')\n",
    "    b = signal.firls(filter_order,np.array([0,low_cutoff-.1, low_cutoff, high_cutoff ,high_cutoff+.5,Fs/2]),np.array([0, 0 ,1 ,1 ,0, 0]),\n",
    "                     np.array([100*0.02,0.02,0.02]),fs=Fs)\n",
    "    X2 = signal.convolve(X1.reshape(-1),b,mode='same')\n",
    "    return X2\n",
    "\n",
    "class heart_rate:\n",
    "    def __init__(self,lower_hr_range=.7,higher_hr_range=3.5,height=.2):\n",
    "        self.hr_now = 0\n",
    "        self.history_hr = []\n",
    "        self.lower_hr_range = lower_hr_range\n",
    "        self.higher_hr_range = higher_hr_range\n",
    "        self.height = height\n",
    "        self.previous = 0\n",
    "        self.step = 2\n",
    "    \n",
    "    def filter_frequencies(self,x_x,f_x,pxx_x):\n",
    "        x_x = np.array(x_x)\n",
    "        x_x = x_x[f_x[x_x]>self.lower_hr_range]\n",
    "        x_x = x_x[f_x[x_x]<self.higher_hr_range]\n",
    "        f_x = f_x[x_x]\n",
    "        pxx_x = pxx_x[x_x]\n",
    "        return f_x,pxx_x\n",
    "    \n",
    "    def get_peaks(self,data,fs,nperseg,nfft):\n",
    "        f_x, pxx_x = signal.welch(data,fs=fs,nperseg=nperseg,nfft=nfft)\n",
    "        f_x = f_x.reshape(-1)\n",
    "        pxx_x = pxx_x/np.max(pxx_x)\n",
    "        x_x, _ = find_peaks(pxx_x, height=self.height)\n",
    "        f,pxx = self.filter_frequencies(x_x,f_x,pxx_x)\n",
    "        ppg = np.array(list(zip(f,pxx)))\n",
    "        if len(ppg)==0:\n",
    "            return []\n",
    "        ppg = ppg[ppg[:,1].argsort()]\n",
    "        return ppg\n",
    "    \n",
    "    def get_rr_value(self,values,acc_window,ecg_rr,Fs=64,nfft=5000):\n",
    "        \"\"\"\n",
    "        Get Mean RR interval\n",
    "\n",
    "        :param values: single channel ppg data\n",
    "        :param Fs: sampling frequency\n",
    "        :param nfft: FFT no. of points\n",
    "        :return: Mean RR interval Information\n",
    "        \"\"\"\n",
    "        values = filter_data(values)\n",
    "        ppg = self.get_peaks(values,Fs,values.shape[0],nfft)[-3:]\n",
    "        \n",
    "        if len(ppg)==0:\n",
    "            if len(self.history_hr)==0:\n",
    "                return 0\n",
    "            self.hr_now = np.mean(self.history_hr)\n",
    "            self.history_hr.append(self.hr_now)\n",
    "            self.history_hr  =self.history_hr[-6:]\n",
    "            return 60000/self.hr_now\n",
    "        \n",
    "        aclx = list(self.get_peaks(acc_window[:,0],Fs//2,acc_window.shape[0],nfft)[-1:])\n",
    "        acly = list(self.get_peaks(acc_window[:,1],Fs//2,acc_window.shape[0],nfft)[-1:])\n",
    "        aclz = list(self.get_peaks(acc_window[:,2],Fs//2,acc_window.shape[0],nfft)[-1:])\n",
    "        acl_unwanted = np.array(aclx+acly+aclz)\n",
    "        if len(acl_unwanted)>0:\n",
    "            acl_unwanted = acl_unwanted[:,0]*60\n",
    "            hr_now = 0\n",
    "            for k in range(1,len(ppg)+1):\n",
    "                hr_temp = ppg[-1*k,0]*60\n",
    "                if len(acl_unwanted[np.where((acl_unwanted>hr_temp-self.step)&(acl_unwanted<hr_temp+self.step))[0]])==0:\n",
    "                    hr_now = hr_temp\n",
    "                    break\n",
    "        else:\n",
    "            hr_now = ppg[-1,0]*60\n",
    "        \n",
    "        if hr_now==0:\n",
    "            hr_now = ppg[-1,0]*60\n",
    "        \n",
    "\n",
    "        if not self.hr_now:\n",
    "            self.hr_now = hr_now\n",
    "            self.history_hr.append(self.hr_now)\n",
    "            self.history_hr  =self.history_hr[-6:]\n",
    "            return 60000/self.hr_now\n",
    "        else:\n",
    "            if abs(self.hr_now-hr_now)>15:\n",
    "                if self.previous>5:\n",
    "                    self.history_hr.append(hr_now)\n",
    "                    self.hr_now = np.mean(self.history_hr)\n",
    "                    self.history_hr.append(self.hr_now)\n",
    "                    self.history_hr  =self.history_hr[-6:]\n",
    "                    self.previous=1\n",
    "                    return 60000/self.hr_now\n",
    "                else:\n",
    "                    self.hr_now = np.mean(self.history_hr)\n",
    "                    self.history_hr.append(self.hr_now)\n",
    "                    self.history_hr = self.history_hr[-6:]\n",
    "                    self.previous+=1\n",
    "                    return 60000/self.hr_now\n",
    "                    \n",
    "            else:\n",
    "                self.hr_now = hr_now\n",
    "                self.history_hr.append(self.hr_now)\n",
    "                self.history_hr  =self.history_hr[-6:]\n",
    "                return 60000/self.hr_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   4 out of  15 | elapsed:  2.2min remaining:  5.9min\n",
      "[Parallel(n_jobs=10)]: Done  12 out of  15 | elapsed:  4.3min remaining:  1.1min\n",
      "[Parallel(n_jobs=10)]: Done  15 out of  15 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../data/PPG_FieldStudy/S13/S13.pkl'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "%matplotlib notebook\n",
    "def get_ecg_rr(ecg_data):\n",
    "    detectors = Detectors(700)\n",
    "    rpeaks = detectors.hamilton_detector(ecg_data[:,1])\n",
    "    ecg_r_ts = ecg_data[np.array(rpeaks),0]\n",
    "    ecg_rr_ts = ecg_r_ts[1:]\n",
    "    ecg_rr_sample = np.diff(ecg_r_ts)\n",
    "    ecg_rr = pd.DataFrame(np.vstack([ecg_rr_ts,ecg_rr_sample]).T,columns=['time','rr'])\n",
    "    ecg_rr['timestamp'] = ecg_rr['time'].apply(lambda a:datetime.utcfromtimestamp(a))\n",
    "    return ecg_rr\n",
    "\n",
    "def bandpass_filter(data,Fs=64,fil_type='ppg'):\n",
    "    X0 = data[:,1]\n",
    "    X1 = signal.detrend(X0,axis=0,type='constant')\n",
    "    X2 = np.zeros((np.shape(X1)[0],data.shape[1]))\n",
    "    nyq = Fs/2\n",
    "    b = signal.firls(219,np.array([0,0.5,0.7,3.5,4,nyq]),\n",
    "                              np.array([0,0,1,1,0,0]),np.array([10,1,1]),fs=nyq*2)\n",
    "    a = [1]\n",
    "    X2[:,0] = data[:,0]\n",
    "    X2[:,1] = signal.filtfilt(b, a, X1)\n",
    "    return X2\n",
    "\n",
    "def bandpass_filter_respiration(data,Fs=700,fil_type='ppg'):\n",
    "    X0 = data[:,1]\n",
    "    X1 = signal.detrend(X0,axis=0,type='constant')\n",
    "    X2 = np.zeros((np.shape(X1)[0],data.shape[1]))\n",
    "    nyq = Fs/2\n",
    "    b = signal.firls(219,np.array([0,0.03,0.05,2,2.5,nyq]),\n",
    "                              np.array([0,0,1,1,0,0]),np.array([10,1,1]),fs=nyq*2)\n",
    "    a = [1]\n",
    "    X2[:,0] = data[:,0]\n",
    "    X2[:,1] = signal.filtfilt(b, a, X1)\n",
    "    return X2\n",
    "\n",
    "def get_quality_features(ppg_data,ppg_fs=64,window_size=2.5):\n",
    "    ppg_data_final = []\n",
    "    n = int(ppg_fs*window_size/2)\n",
    "    for i in range(n,ppg_data.shape[0]-n,1):\n",
    "        tmp = []\n",
    "        tmp.append(ppg_data[i,0])\n",
    "        tmp.append(ppg_data[i,1])\n",
    "        tmp.extend([-1]*4)\n",
    "#         sample = ppg_data[(i-n):(i+n),1]\n",
    "#         tmp.append(skew(sample))\n",
    "#         tmp.append(kurtosis(sample))\n",
    "#         tmp.append(iqr(sample))\n",
    "#         f,pxx = signal.welch(sample,fs=ppg_fs,nperseg=len(sample)//2,nfft=10000,axis=0)\n",
    "#         tmp.append(np.trapz(pxx[np.where((f>=.8)&(f<=2.5))[0]])/np.trapz(pxx))\n",
    "        ppg_data_final.append(np.array(tmp))\n",
    "    return np.array(ppg_data_final)\n",
    "\n",
    "\n",
    "def save_participant_data(filename,ecg_fs = 700,ppg_fs = 64,acc_fs=32,window_size=8,activity_fs=4):\n",
    "    data = pickle.load(open(filename,'rb'),encoding='latin1')\n",
    "    ppg_data = data['signal']['wrist']['BVP']\n",
    "    acc_data = data['signal']['wrist']['ACC']/64\n",
    "    activity_data = data['activity']\n",
    "    label_data = data['label']\n",
    "    ecg_data = data['signal']['chest']['ECG']\n",
    "    respiration_data = data['signal']['chest']['Resp']\n",
    "    total_seconds = ppg_data.shape[0]/ppg_fs\n",
    "    start_ts = datetime.utcnow().timestamp()\n",
    "    ecg_ts = start_ts + np.arange(0,total_seconds,1/ecg_fs)\n",
    "    acc_ts = start_ts + np.arange(0,total_seconds,1/acc_fs)\n",
    "    label_ts = start_ts + np.arange(0,label_data.shape[0]*2,1/.5) + 4\n",
    "    print(label_data.shape,label_ts.shape)\n",
    "    label_data = np.concatenate([label_ts.reshape(-1,1),label_data.reshape(-1,1)],axis=1)\n",
    "    print(label_data.shape,label_ts.shape,'after')\n",
    "    activity_ts = start_ts + np.arange(0,total_seconds,1/activity_fs)\n",
    "    activity_data = np.concatenate([activity_ts.reshape(-1,1),activity_data],axis=1)\n",
    "    acc_data = np.concatenate([acc_ts.reshape(-1,1),acc_data],axis=1)\n",
    "    respiration_ts = ecg_ts\n",
    "    respiration_data = np.vstack([respiration_ts,respiration_data.reshape(-1)]).T\n",
    "    ecg_data = np.vstack([ecg_ts,ecg_data.reshape(-1)]).T\n",
    "    print(ecg_data.shape[0]/ecg_fs,respiration_data.shape[0]/ecg_fs,\n",
    "          acc_data.shape[0]/acc_fs,activity_data.shape[0]/activity_fs,\n",
    "          label_data.shape[0]*2)\n",
    "    ecg_rr1 = get_ecg_rr(ecg_data)\n",
    "    ecg_rr = ecg_rr1.values\n",
    "    ppg_ts = start_ts + np.arange(0,total_seconds,1/ppg_fs)\n",
    "    ppg_data = np.vstack([ppg_ts,ppg_data.reshape(-1)]).T\n",
    "    print(len(np.array(range(0,ppg_data.shape[0]-window_size*ppg_fs//2-window_size*ppg_fs//4,window_size*ppg_fs//4))),label_data.shape)\n",
    "    ppg_data = bandpass_filter(ppg_data,Fs=ppg_fs,fil_type='ppg')\n",
    "    respiration_data = bandpass_filter_respiration(respiration_data,Fs=ecg_fs,fil_type='ppg')\n",
    "    respiration_data[:,0] = respiration_data[:,0]*1000\n",
    "    peak_index,valley_index = compute_peak_valley(respiration_data)\n",
    "    peak_data = respiration_data[peak_index]\n",
    "    valley_data = respiration_data[valley_index]\n",
    "    rip_feature = rip_cycle_feature_computation(peak_data,valley_data)[:,:5]\n",
    "    rip_feature[:,:2] = rip_feature[:,:2]/1000\n",
    "    ppg_data = get_quality_features(ppg_data)\n",
    "    ppg_data = pd.DataFrame(ppg_data,columns=['time','ppg','skew','kurtosis','iqr','relative_power']).dropna().sort_values('time').reset_index(drop=True)\n",
    "    respiration_data[:,0] = respiration_data[:,0]/1000\n",
    "    all_data = []\n",
    "    count = 0\n",
    "    for i in range(0,ppg_data.shape[0]-window_size*ppg_fs//2-window_size*ppg_fs//4,window_size*ppg_fs//4):\n",
    "        a = ppg_data.loc[i:i+window_size*ppg_fs-1]\n",
    "        b = respiration_data[np.where((respiration_data[:,0]>=a['time'].min())&(respiration_data[:,0]<a['time'].max()))[0],1].reshape(-1,1)\n",
    "        hr = label_data[count,1]\n",
    "        count+=1\n",
    "        if count==label_data.shape[0]:\n",
    "            continue\n",
    "        all_data.append([a['time'].min(),a['time'].max(),\n",
    "                         a[['time',\n",
    "                            'ppg',\n",
    "                            'skew',\n",
    "                            'kurtosis',\n",
    "                            'iqr',\n",
    "                            'relative_power']].sort_values('time').reset_index(drop=True),b,hr])\n",
    "#     print(len(np.array(range(0,ppg_data.shape[0]-window_size*ppg_fs//4,window_size*ppg_fs//4))))\n",
    "    \n",
    "    ppg_windows = pd.DataFrame(all_data,columns=['start_time','end_time','data','respiration','hr'])\n",
    "    ppg_windows['ecg_rr'] = ppg_windows.apply(lambda a:np.mean(ecg_rr[np.where((ecg_rr[:,0]>=a['start_time'])&(ecg_rr[:,0]<a['end_time']))[0],1]),axis=1)\n",
    "    ppg_windows['inspiration_duration'] = ppg_windows.apply(lambda a:np.mean(rip_feature[np.where((rip_feature[:,1]>=a['start_time'])&(rip_feature[:,0]<a['end_time']))[0],2]),axis=1)\n",
    "    ppg_windows['expiration_duration'] = ppg_windows.apply(lambda a:np.mean(rip_feature[np.where((rip_feature[:,1]>=a['start_time'])&(rip_feature[:,0]<a['end_time']))[0],3]),axis=1)\n",
    "    ppg_windows['respiration_duration'] = ppg_windows.apply(lambda a:np.mean(rip_feature[np.where((rip_feature[:,1]>=a['start_time'])&(rip_feature[:,0]<a['end_time']))[0],4]),axis=1)\n",
    "    ppg_windows['acc_window'] = ppg_windows.apply(lambda a: acc_data[np.where((acc_data[:,0]>=a['start_time'])&(acc_data[:,0]<a['end_time']))[0],:],axis=1)\n",
    "    ppg_windows['label'] = ppg_windows.apply(lambda a: activity_data[np.where((activity_data[:,0]>=a['start_time'])&(activity_data[:,0]<a['end_time']))[0],1],axis=1)\n",
    "    ppg_windows['label'] = ppg_windows['label'].apply(lambda a:mode(a)[0][0])\n",
    "    \n",
    "    print(ppg_windows.shape)\n",
    "    ppg_windows = ppg_windows.sort_values('start_time').reset_index(drop=True)\n",
    "    \n",
    "    hr = heart_rate()\n",
    "    ppg_rr_col = []\n",
    "    for i,a in ppg_windows.iterrows():\n",
    "        ppg_rr_col.append(hr.get_rr_value(a['data'].sort_values('time').reset_index(drop=True)['ppg'].values,\n",
    "                                        a['acc_window'][a['acc_window'][:,0].argsort(),1:],\n",
    "                                        a['ecg_rr']*1000))\n",
    "        \n",
    "        \n",
    "    ppg_windows['ppg_rr'] = np.array(ppg_rr_col)/1000\n",
    "    \n",
    "    if not os.path.isdir(output_directory+str(window_size)+'d'):\n",
    "        os.makedirs(output_directory+str(window_size)+'d')\n",
    "    final_path = output_directory+str(window_size)+'d/'\n",
    "    participant_name = filename.split('/')[-1]\n",
    "    pickle.dump(ppg_windows,open(final_path+participant_name,'wb'))\n",
    "    \n",
    "from joblib import Parallel,delayed\n",
    "output_directory = '../data/'\n",
    "window_size = 20\n",
    "final = Parallel(n_jobs=10,verbose=2)(delayed(save_participant_data)(f,window_size=window_size) for f in filelists)\n",
    "# final = [save_participant_data(f,window_size=20) for f in filelists[:1]]\n",
    "# for filename in filelists:\n",
    "filelists[0]\n",
    "#     print(ecg_rr.head(),ppg_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25839, 640, 4) (25839,) (25839,) (25839,) (25839,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "from collections import Counter\n",
    "\n",
    "def get_individual_data(window_size = 20,ppg_fs = 64):\n",
    "    filelists = ['../data/'+str(window_size)+'d/'+a for a in os.listdir('../data/'+str(window_size)+'d/') \n",
    "                 if a[-1] not in ['s','f']]\n",
    "    X_acl = []\n",
    "    X_ppg = []\n",
    "    y_hr = []\n",
    "    y_participant = []\n",
    "    y_activity = []\n",
    "    y_time = []\n",
    "    data_col = []\n",
    "    activity_dict = {0:'No Label',1:'Sitting',2:'Stairs',3:'Soccer',\n",
    "                    4:'Cycling',5:'Driving',6:'Lunch',7:'Walking',8:'Working'}\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    for i in range(len(filelists)):\n",
    "        data = pickle.load(open(filelists[i],'rb'))\n",
    "        data['shape'] = data['data'].apply(lambda a:a.shape[0])\n",
    "        data = data[data['shape']==ppg_fs*window_size]\n",
    "        data['ppg_window'] = data['data'].apply(lambda a:\n",
    "                                                a.sort_values('time').reset_index(drop=\n",
    "                                                True)['ppg'].values.reshape(ppg_fs*window_size,\n",
    "                                                1)[np.arange(0,ppg_fs*window_size,2),:])\n",
    "        data['ppg_window'] = data['ppg_window'].apply(lambda a:MinMaxScaler().fit_transform(a.reshape(ppg_fs*window_size//2,1)))\n",
    "        data['ppg_window'] = data['ppg_window'].apply(lambda a:a.reshape(1,ppg_fs*window_size//2,1))\n",
    "        data['acc_window'] = data['acc_window'].apply(lambda a:a.reshape(1,ppg_fs*window_size//2,4))\n",
    "        data['ecg_rr'] = data['ecg_rr'].apply(lambda a:60/a)\n",
    "\n",
    "        y_time.extend(list(data['start_time'].values))\n",
    "        X_acl.extend(list(data['acc_window'].values))\n",
    "        X_ppg.extend(list(data['ppg_window'].values))\n",
    "        y_hr.extend(list(data['ecg_rr'].values))\n",
    "        y_participant.extend([i]*data.shape[0])\n",
    "        y_activity.extend(list(data['label'].values))\n",
    "    X_acl = np.concatenate(X_acl)[:,:,1:]\n",
    "    X_ppg = np.concatenate(X_ppg)\n",
    "    X = np.concatenate([X_ppg,X_acl],axis=2)\n",
    "    y_hr = np.array(y_hr)\n",
    "    y_participant = np.array(y_participant)\n",
    "    y_activity = np.array(y_activity)\n",
    "    y_time = np.array(y_time)\n",
    "    print(X.shape,y_hr.shape,y_participant.shape,y_activity.shape,y_time.shape)\n",
    "    return X,y_hr,y_participant,y_activity,y_time\n",
    "window_size = 20\n",
    "a = get_individual_data(window_size = window_size,ppg_fs = 64)\n",
    "pickle.dump(a,open('../data/dalia_individual_windows_'+str(window_size)+'.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5., 6., 7., 8.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(a[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "filelists = ['../data/8d/'+a for a in os.listdir('../data/8d/') if a[-1] not in ['s','f']]\n",
    "from datetime import datetime\n",
    "X_acl = []\n",
    "X_ppg = []\n",
    "y_hr = []\n",
    "y_participant = []\n",
    "y_activity = []\n",
    "y_time = []\n",
    "data_col = []\n",
    "activity_dict = {0:'No Label',\n",
    "                1:'Sitting',\n",
    "                2:'Stairs',\n",
    "                3:'Soccer',\n",
    "                4:'Cycling',\n",
    "                5:'Driving',\n",
    "                6:'Lunch',\n",
    "                7:'Walking',\n",
    "                8:'Working'}\n",
    "\n",
    "activity_dict1 = {'No Label':-1,\n",
    "                'Sitting':0,\n",
    "                'Stairs':1,\n",
    "                'Soccer':2,\n",
    "                'Cycling':3,\n",
    "                'Driving':4,\n",
    "                'Lunch':-1,\n",
    "                'Walking':5,\n",
    "                'Working':-1}\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "for i in range(len(filelists)):\n",
    "    data = pickle.load(open(filelists[i],'rb'))\n",
    "    data['shape'] = data['data'].apply(lambda a:a.shape[0])\n",
    "    data = data[data['shape']==512]\n",
    "    data['ppg_window'] = data['data'].apply(lambda a:a.sort_values('time').reset_index(drop=True)['ppg'].values.reshape(512,1)[np.arange(0,512,2),:])\n",
    "    data['shape'] = data['ppg_window'].apply(lambda a:a.shape[0])\n",
    "    data = data[data['shape']==256]\n",
    "    data['shape'] = data['acc_window'].apply(lambda a:a.shape[0])\n",
    "    data = data[data['shape']==256]\n",
    "#     data['ppg_window'] = data['ppg_window'].apply(lambda a:MinMaxScaler().fit_transform(a.reshape(256,1)))\n",
    "    data['ppg_window'] = data['ppg_window'].apply(lambda a:a.reshape(1,256,1))\n",
    "    data['acc_window'] = data['acc_window'].apply(lambda a:a.reshape(1,256,4))    \n",
    "#     data['activity'] = data['label'].apply(lambda a:mode(list(a))[0][0])\n",
    "    data['activity_label'] = data['label'].apply(lambda a:activity_dict[a])\n",
    "    data = data[data.label>0]\n",
    "    y_time.extend(list(data['start_time'].values))\n",
    "    X_acl.extend(list(data['acc_window'].values))\n",
    "    X_ppg.extend(list(data['ppg_window'].values))\n",
    "    y_hr.extend(list(data['hr'].values))\n",
    "    y_participant.extend([i]*data.shape[0])\n",
    "    y_activity.extend(list(data['label'].values))    \n",
    "\n",
    "X_acl = np.concatenate(X_acl)[:,:,1:]\n",
    "X_ppg = np.concatenate(X_ppg)\n",
    "X = np.concatenate([X_ppg,X_acl],axis=2)\n",
    "y_hr = np.array(y_hr)\n",
    "y_participant = np.array(y_participant)\n",
    "y_activity = np.array(y_activity)\n",
    "y_time = np.array(y_time)\n",
    "pickle.dump([X,y_hr,y_participant,y_activity,y_time],open('../data/dalia_individual_windows.p','wb'))\n",
    "\n",
    "X.shape,y_hr.shape,y_participant.shape,y_activity.shape,y_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64247, 30) (64247,) (64247,) (64247, 30)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "filelists = ['../data/8d/'+a for a in os.listdir('../data/8d/') if a[-1] not in ['s','f']]\n",
    "from datetime import datetime\n",
    "y_stress = []\n",
    "y_participant = []\n",
    "X_hr = []\n",
    "X_time = []\n",
    "X_acl = []\n",
    "import pickle\n",
    "for i,f in enumerate(filelists):\n",
    "    data = pickle.load(open(f,'rb'))\n",
    "    data['ppg_rr'] = (data['ppg_rr'] - data['ppg_rr'].mean())/data['ppg_rr'].std()\n",
    "    data['acl_std'] = data['acc_window'].apply(lambda a:np.sqrt(np.sum(np.square(np.std(a,axis=0)))))\n",
    "    data = data.sort_values('start_time').reset_index(drop=True)\n",
    "    data['timestamp'] = data['start_time'].apply(lambda a:datetime.utcfromtimestamp(a))\n",
    "    for j in range(0,data.shape[0],1):\n",
    "        temp = data[j:(j+30)]\n",
    "        if temp.shape[0]!=30:\n",
    "            continue\n",
    "        labels = temp['label'].values\n",
    "        y_stress.append(mode(labels)[0][0])\n",
    "        X_hr.append(temp['ppg_rr'].values.reshape(1,30))\n",
    "        X_acl.append(temp['acl_std'].values.reshape(1,30))        \n",
    "        y_participant.append(i)\n",
    "        X_time.append(temp['start_time'].values[0])\n",
    "\n",
    "y_stress = np.array(y_stress)\n",
    "y_participant = np.array(y_participant)\n",
    "X_time = np.array(X_time)\n",
    "X_hr = np.concatenate(X_hr)\n",
    "X_acl = np.concatenate(X_acl)\n",
    "print(X_acl.shape,y_stress.shape,y_participant.shape,X_hr.shape)\n",
    "\n",
    "import pickle\n",
    "pickle.dump([X_hr,X_time,y_participant,X_acl],open('../data/tabular_data_60_seconds_ppg_rr_dalia_normalized.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from scipy.stats import mode\n",
    "data_col = []\n",
    "activity_dict = {0:'No Label',\n",
    "                1:'Sitting',\n",
    "                2:'Stairs',\n",
    "                3:'Soccer',\n",
    "                4:'Cycling',\n",
    "                5:'Driving',\n",
    "                6:'Lunch',\n",
    "                7:'Walking',\n",
    "                8:'Working'}\n",
    "activity_dict1 = {'No Label':-1,\n",
    "                'Sitting':0,\n",
    "                'Stairs':1,\n",
    "                'Soccer':2,\n",
    "                'Cycling':3,\n",
    "                'Driving':4,\n",
    "                'Lunch':-1,\n",
    "                'Walking':5,\n",
    "                'Working':-1}\n",
    "\n",
    "X_acl = []\n",
    "X_ppg = []\n",
    "y_hr = []\n",
    "y_participant = []\n",
    "y_activity = []\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "for i in range(len(filelists)):\n",
    "    data = pickle.load(open(filelists[i],'rb'))\n",
    "#     data['acc_window'] = data['acc_window'].apply(lambda a:a)\n",
    "    data['shape'] = data['data'].apply(lambda a:a.shape[0])\n",
    "    data = data[data['shape']==512]\n",
    "    data['ppg_window'] = data['data'].apply(lambda a:a.sort_values('time').reset_index(drop=True)['ppg'].values.reshape(512,1)[np.arange(0,512,2),:])\n",
    "    data['shape'] = data['ppg_window'].apply(lambda a:a.shape[0])\n",
    "    data = data[data['shape']==256]\n",
    "    data['shape'] = data['acc_window'].apply(lambda a:a.shape[0])\n",
    "    data = data[data['shape']==256]\n",
    "    data['ppg_window'] = data['ppg_window'].apply(lambda a:MinMaxScaler().fit_transform(a.reshape(256,1)))\n",
    "    data['ppg_window'] = data['ppg_window'].apply(lambda a:a.reshape(1,256,1))\n",
    "    data['acc_window'] = data['acc_window'].apply(lambda a:a.reshape(1,256,4))    \n",
    "#     print(Counter(data['shape'].values))\n",
    "    data['activity'] = data['activity'].apply(lambda a:mode(list(a))[0][0])\n",
    "    data['activity_label'] = data['activity'].apply(lambda a:activity_dict[a])\n",
    "#     data['activity'] = data['activity_label'].apply(lambda a:activity_dict1[a])\n",
    "    data = data[data.activity>0]\n",
    "    X_acl.extend(list(data['acc_window'].values))\n",
    "    X_ppg.extend(list(data['ppg_window'].values))\n",
    "    y_hr.extend(list(data['hr'].values))\n",
    "    y_participant.extend([i]*data.shape[0])\n",
    "    y_activity.extend(list(data['activity_label'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
